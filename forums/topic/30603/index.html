<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8" />
  <title>CPU pipelining and out-of-order execution - Forums - ASM Community</title>
  <link rel="stylesheet" type="text/css" href="../../../style.css" />
  <link rel="canonical" href="../?id=30603" />
     </head>
 <body>
  <div id="header">
   <h1><a href="../../../">ASM Community</a></h1>
  </div>
  <div id="content">
   <p class="breadcrumbs"><a href="../../../">Home</a> &raquo; <a href="../../">Forums</a> &raquo; <a href="../../board/?id=113">Low Level Concepts</a> &raquo; <a href="../?id=30603">CPU pipelining and out-of-order execution</a></p>
   <div class="post" id="post-214668">
    <div class="subject"><a href="#post-214668">CPU pipelining and out-of-order execution</a></div>
    <div class="body">While going through some stuff on my webserver, I came across this old article that I wrote, trying to explain some of the basics of CPU pipelining and out-of-order execution.<br />It&#039;s quite old, written back in the early days of OoO, with the Pentium Pro... But there haven&#039;t really been major changes in the execution model of CPUs since the Pentium Pro, so it may still be quite useful to people today. So I thought I&#039;d post it here.<br />======<br /><br />An execution pipeline is basically a conveyor belt for instructions.<br />Instructions pass various stages, at each stage, a part of the execution of<br />the instruction is done. Normally each stage will take 1 clock cycle (clk),<br />that is, an instruction will stay in a stage for 1 clk, and then proceed on to<br />the next.<br />Basically there are 3 main operations:<br /><br />- Fetch: Load instruction from memory.<br />- Decode: Set up operands (load from memory if necessary), and find out what<br />the instruction is supposed to do. Basically prepare to dispatch operands to<br />the correct logic unit, for execution.<br />- Execute: Take the operands, dispatch to the correct logic unit, perform the<br />desired operation, and store the result.<br /><br />A simplified pipeline could have these three stages: fetch - decode - execute.<br />The instruction passes through them in that sequence:<br /><br /><br /><pre><code>&nbsp; &nbsp; &nbsp; &nbsp; Fetch -&gt; Decode -&gt; Execute<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 0: | INSTR&nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br /><br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 1: |&nbsp; &nbsp; &nbsp; &nbsp; | INSTR&nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br /><br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 2: |&nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; | INSTR&nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+</code></pre><br /><br />Now if the instruction is being fetched, it cannot be decoded or executed yet.<br />Does this mean that only 1 stage of the pipeline is active, while the rest is<br />idle?<br />The answer is no. Why not? Because while you execute the first instruction,<br />you can decode the second, and fetch the third at the same time. The different<br />stages do not share any hardware resources, so each stage can work<br />independently of the others, at the same time:<br /><br /><pre><code>&nbsp; &nbsp; &nbsp; &nbsp; Fetch -&gt; Decode -&gt; Execute<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 0: | INSTR0 |&nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br /><br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 1: | INSTR1 | INSTR0 |&nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br /><br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 2: | INSTR2 | INSTR1 | INSTR0 |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+</code></pre><br /><br />So the stages can work in parallel on sequential instructions. This in effect<br />reduces the execution time of a sequence of instructions. If you have for<br />example a piece of code of 10 instructions, it doesn&#039;t take 10 x 3 = 30 clks,<br />but instead, it takes 2 clks for the first instruction to reach the execute<br />stage, and from there on, at every clk a new instruction arrives at the<br />execute stage, so you execute the 10 instructions in 10 clks. That&#039;s a total<br />of only 2 + 10 = 12 clks. There is a sort of &#039;telescoping&#039;, or &#039;waterfall&#039; effect.<br /><br />The different instructions in the pipeline can be dependent however. It could<br />be that the instruction that is being decoded, requires an operand that is the<br />result of the instruction being executed. To solve this problem, operand<br />forwarding was introduced. This means that the decode stage does not have to<br />wait for the result to be available. The result is forwarded back into the<br />pipeline as a new operand, so execution can continue without &quot;stalling&quot;<br />(waiting):<br /><br />Consider an add instruction of the form: add destination, source1, source2<br /><br />INSTR0: add r0, r1, r2<br />INSTR1: add r4, r0, r3 &lt;- dependency, r0 is modified by the previous<br />instruction<br /><br /><pre><code>&nbsp; &nbsp; &nbsp; &nbsp; Fetch -&gt; Decode -&gt; Execute<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 0: | INSTR2 | INSTR1 | INSTR0 |-&gt;-+<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+&nbsp;  |<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  +------&lt;-----+ r0<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 1: | INSTR3 | INSTR2 | INSTR1 |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+</code></pre><br /><br />Another problem arises when a jump is made. When you jump to a different<br />location, the instructions that were already in the pipeline, are now invalid.<br />So the pipeline needs to be flushed, and start over at the right location.<br />With unconditional jumps, modern CPUs will spot the jumps in time, and<br />continue fetching instructions at the new location right away, without<br />stalling.<br />With conditional jumps however, this is not possible. It is not clear whether<br />the jump should be taken or not, until the condition has been tested, which at<br />worst happens by the last instruction before the conditional jump in the<br />pipeline. So instead of waiting, a modern CPU will try to predict whether the<br />jump will be taken or not, judging from a hint in the code (the usual rule is:<br />if the jump is to a lower address, it is probably a loop, so take the jump.<br />Else, do not take it. Some CPUs even have hint bits inside the opcode, to<br />indicate the behaviour), and from previous iterations. They store whether the<br />jump was taken or not, on the last few occasions. With this strategy, the code<br />can usually continue without stalling. Only when the prediction turns out to<br />be wrong, the pipeline will have to be flushed, and execution will stall for a<br />few clks until the first instruction has reached the execution stage again.<br /><br />A superscalar CPU is a CPU which has more than one execution pipeline, working<br />in parallel.<br />So you can execute more than one instruction per clk. Operand forwarding can<br />not solve all dependency problems anymore however. It could be that there are<br />2 instructions being executed in parallel, but the second one requires the<br />result of the first one. The CPU will then have to put the second pipeline on<br />hold. It can only execute the first instruction, then forward the result into<br />the second pipeline. Then the second pipeline can continue executing on the<br />next clk.<br />We refer to this as a stall in the second pipeline. The penalty was 1 clk.<br />It is the responsibility of the programmer to avoid these stalls.<br /><br />What happens when the clockspeed goes up?<br />All stages in the pipeline consist of sequences of logic gates. A gate has a<br />certain propagation time. The time it takes to settle to a consistent state,<br />after a change of input signals. So the sequence of gates in a pipeline stage<br />can have at most a propagation time of 1 clk. As clockspeed goes up, the time<br />that 1 clk takes goes down, so the maximum length of the logic sequence in a<br />pipeline stage decreases aswell.<br />There are 2 solutions to this problem:<br /> - Reduce the logic required to fetch, decode and execute instructions<br /> - Split the pipeline up in more stages<br /><br />In order to reduce the logic required to fetch and decode instructions, the<br />instructions should be encoded in a less complex fashion.<br />Reduced Instruction Set Computing (RISC) is a CPU design philosophy which aims<br />for small and simple instructionsets. Complex instructions which can be<br />replaced by a sequence of 1 or more simple instructions without significantly<br />increasing total execution time, will not be implemented in hardware.<br />Instructions should also be encoded in an orthogonal (uniform) manner, to<br />reduce decoding complexity. They should all be the same size, and layout.<br />The old paradox of &quot;less is more&quot; is a nice description of RISC.<br />To give a name to the previous, more complex designs, the name Complex<br />Instruction Set Computing (CISC) was chosen. These more complex designs date<br />from an earlier age, when memory was small and expensive, and clock speeds<br />were low. There was no pipelining yet, let alone superscalar designs. Code was<br />mostly written by hand, rather than by compilers. So the designs were made to<br />minimize the code footprint, and maximize the operations per cycle. This meant<br />a lot of instructions with implicit operands, and performing common sequences<br />of operations.<br /><br />A few results of this philosophy are that virtually all instructions can only<br />operate on registers, and virtually all instructions have the same number of<br />operands.<br />Loading registers from memory, or storing to memory is done with a few special<br />instructions.<br />At the time of RISC implementation, manufacturers were able to put millions of<br />gates on a chip, but because of RISC, way less were needed than ever before.<br />The extra gates could now be used for more registers, extra cache or other<br />features.<br /><br />Splitting the pipeline up in order to increase clockspeed is a poor mans<br />solution. Namely, if there are interlocks because of dependent instructions<br />for example, the penalties can increase from 1 to several stages. But, what if<br />you cannot redesign your instructionset, because you want to keep supporting<br />legacy code?<br /><br />There are again 2 solutions:<br /> - Design a new RISC CPU, and support legacy code with a software emulation<br />layer.<br /> - Design a new RISC CPU, and support legacy code with a hardware emulation<br />layer.<br /><br />It is interesting to see that Motorola chose the first option, when going from<br />68k to PowerPC.<br />The Apple computers were powered by 68k CPUs. The new PowerPC-powered CPUs had<br />68k emulation built into the OS. At that time, the choice was not such a bad<br />one. The new RISC CPU could be manufactured at much higher clockspeeds, and<br />with more cache and more registers. This gave a leap in performance, which<br />could make emulated legacy code perform acceptably, while native code would<br />run at the highest possible speed.<br /><br />Motorola did however release a 68060, which was designed with the second<br />option. Which was the same choice that Intel made for their x86 family. The<br />instructionset was not redesigned totally. They did use some ideas taken from<br />the RISC philosophy however. In short, their CPUs now translate the complex<br />x86 instructions internally, and then issue them to an execution pipeline<br />which could be seen as a sort of RISC CPU. It can execute what Intel calls<br />&quot;micro-operations&quot; or µOps, since the Pentium PRO (Pentium PRO, Celeron, PII<br />and PIII all have the same P6 core). And surely, some of the instructions<br />which would have been removed in a RISC instructionset, are now constructed<br />from a sequence of 2 or more µOps. The Pentium has no names for the &#039;atomic<br />operations&#039; that it builds the instructions from... But it clearly does just<br />that.<br />Their pipeline is still longer than that of a true RISC CPU, but it is shorter<br />than it would be when all instructions were to be decoded and executed<br />directly, rather than translated to µOps.<br />Intel managed to keep the implementation of a complex instructionset within<br />reasonable bounds, and managed to get the clockspeed up to impressive rates.<br />At the time of writing they have a Pentium 4 CPU out, which runs at 1.5 GHz.<br />This makes the Pentium 4 the highest clocked CPU currently available.<br />The high clockspeed comes at a cost however. The instruction throughput is<br />relatively low, because the pipeline has a lot of stages, because at 1.5 GHz,<br />the stages have to be very short. On top of that, its decoding scheme is still<br />more complex than that of CPUs with a RISC instructionset.<br /><br />But let&#039;s look at the x86 family more closely. Because at the backend there is<br />the RISC-like µOp-execution unit, the CPU also gets RISC-like traits. What<br />does this mean?<br />The translation from x86 instructions to µOps is not always an easy one. Only<br />a part of the x86 instructionset can be considered orthogonal. But the x86<br />instructions have complex addressing modes. Intel decided to map register-only<br />operations 1:1 to µOps, and add additional µOps for the complex addressing<br />modes. Pentium PRO can handle simple addressing modes () in 1 µOp forms<br />aswell.<br /><br />A few Pentium examples:<br /><br /><pre><code>&nbsp; &nbsp; add eax, edx</code></pre><br /><br />is 1 clk.<br /><br /><pre><code>&nbsp; &nbsp; add eax, </code></pre><br />&nbsp;  <br />will translate to:<br /><br /><pre><code>&nbsp; &nbsp; mov tmp, <br />&nbsp; &nbsp; add eax, tmp</code></pre><br />&nbsp;  <br />where tmp is a temporary internal register. Takes 2 clks.<br /><br /><pre><code>&nbsp; &nbsp; add , edx</code></pre><br />&nbsp;  <br />is three operations on Pentium:<br /><br /><pre><code>&nbsp; &nbsp; mov tmp, <br />&nbsp; &nbsp; add tmp, edx<br />&nbsp; &nbsp; mov , tmp</code></pre><br />&nbsp;  <br />This is exactly the style in which RISC code is written, since add would only<br />be able to operate on registers, not on memory directly.<br />So in short, the CPU seems to translate ordinary x86 code to RISC code.<br />The downside to having the CPU translate your code, is that the pipeline will<br />have to wait for the sequence of instructions to finish, before issuing the<br />next pair of instructions.<br />When you write the RISC-like code yourself, you are working with 1 clk<br />operations only, and you can get 2 independent instructions through per clk.<br />A few instructions are even more interesting on Pentium, because translating<br />them by hand will actually make them faster.<br /><br />For example:<br /><br /><pre><code>&nbsp; &nbsp; cdq</code></pre><br />&nbsp;  <br />takes 3 clks.<br /><br /><pre><code>&nbsp; &nbsp; mov edx, eax<br />&nbsp; &nbsp; sar edx, 31</code></pre><br />&nbsp;  <br />takes 2 clks.<br /><br /><pre><code>&nbsp; &nbsp; stosd</code></pre><br />&nbsp;  <br />takes 2 clks.<br /><br /><pre><code>&nbsp; &nbsp; mov , eax<br />&nbsp; &nbsp; add esi, 4</code></pre><br />&nbsp;  <br />takes 1 clk.<br /><br /><pre><code>&nbsp; &nbsp; loop label</code></pre><br /><br />takes 5 clks.<br /><br /><pre><code>&nbsp; &nbsp; dec ecx<br />&nbsp; &nbsp; jnz label</code></pre><br />&nbsp;  <br />takes 1 clk.<br /><br />Basically there are a few extra rules for instructions to execute in 1 clk on<br />Pentium.<br />The Pentium has 2 pipelines, but they are not identical, the primary pipeline<br />is called U, the secondary pipeline is called V. Some instructions can only be<br />executed in U (eg. shift/rotate instructions), others only in V (eg. call/jump<br />instructions).<br />There are also some instructions which will not pair at all. They will only<br />execute in U, and V will stall.<br />The decoders also have limits. Each pipeline can decode instructions up to 7<br />bytes per clk.<br />If a longer instruction is encountered, there will be a stall.<br />Finally, the Address Generation Unit (AGU) is a stage earlier in the pipeline.<br />What this means is that if you have an instruction that has to address memory<br />with the result of a previous instruction, then it will have to stall for 1<br />clk. Namely, the operand has to be forwarded not one, but 2 stages back. This<br />is known as the Address Generation Interlock (AGI) stall:<br /><br /><pre><code>&nbsp; &nbsp; add eax, ebx<br />&nbsp; &nbsp; mov edx, <br /><br />&nbsp; &nbsp; &nbsp; &nbsp; Decode -&gt;&nbsp; AGU -&gt; Execute<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 0: | INSTR2 | INSTR1 | INSTR0 |-&gt;-+<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+&nbsp;  |<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; +----------&lt;----------+ eax<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+<br />clk 1: | INSTR2 | INSTR1 |&nbsp; &nbsp; &nbsp; &nbsp; |<br />&nbsp; &nbsp; &nbsp;  +--------+--------+--------+</code></pre><br />We say the latency of the instruction is 2 clks. The result has to be<br />available 2 clks ahead. Most instructions have a latency of 1 clk. Operand<br />forwarding can avoid stalls on 1 clk latency instructions, as long as there<br />are no 2 dependent instructions in the pipeline at the same time, as we&#039;ve<br />seen earlier.<br /><br />The Pentium PRO (and other P6 core CPUs) is a completely different beast<br />however. Instead of translating in-place, as the Pentium does, the Pentium PRO<br />decodes the x86 instructions into µOps and stores them in a buffer. A<br />scheduling unit (Reservation Station) will then dispatch µOps to the units as<br />the operands and the unit become available. This means it can execute<br />instructions out-of-order (ooo). The results are then stored in a reorder<br />buffer (ROB), so the result is guaranteed to be equivalent to the original<br />sequence of instructions. In other words, the out-of-order execution is<br />transparent to the program.<br />Why is this interesting? A stall occuring in the early part of the pipeline<br />will not affect the backend. The decoder might not issue new µOps during the<br />stall, but while there are still µOps in the ooo buffer, the backend can<br />continue execution. The ooo execution can also minimize the cost of<br />latencies/stalls. While 1 µOp has to wait on a previous one to complete, the<br />CPU can still dispatch other µOps which are not dependent, even if they were<br />originally sequenced after the dependent instruction.</div>
    <div class="meta">Posted on 2011-07-21 11:56:29 by Scali</div>
   </div>
   <div class="post" id="post-214669">
    <div class="subject"><a href="#post-214669">Re: CPU pipelining and out-of-order execution</a></div>
    <div class="body">(continued from previous post)<br /><br />The Pentium PRO has 3 decoders, d0, d1 and d2. Decoder d0 can decode an<br />instruction with a maximum length of 7 bytes, and a maximum complexity of 4<br />µOps per clk. Decoders d1 and d2 can decode an additional instruction of 1 µOp<br />per clk each.<br />So the ooo buffer can be filled with up to 6 µOps per clk.<br /><br />The ooo execution pipeline has 5 different ports, port 0 through 4. Each port<br />handles a specific class of µOps.<br />Port 0 handles integer and floating point arithmetic, and address generation<br />operations.<br />Port 1 handles simple integer arithmetic instructions (not shift, multiply or<br />divide).<br />Port 2 handles memory load operations.<br />Port 3 handles the calculation of memory write addresses.<br />Port 4 handles memory write operations.<br /><br />Each port can take 1 µOp per clk. This means that the CPU can dispatch a<br />maximum of 5 (independent) µOps per clk. Since the decoders decode into the<br />ooo buffer, they are now no longer dependent on the execution backend. This<br />means that the decoders no longer have to stall when the execution takes more<br />than 1 clk. In fact, the ports on a PPRO themselves are pipelined aswell.<br />Which means that even multiple-clk instructions such as multiply can be<br />issued every clk, and they pass through the stages subsequently. With PPRO,<br />you don&#039;t specify execution speed of instructions by simply counting the<br />clks, instead, you specify latency and throughput.<br />For example, mul is specified as: latency 4 clks, throughput 1 per clk.<br />This means that when we issue a mul, it takes 4 clks to complete in total.<br />But you can issue 1 mul per clk. If you issue 2 subsequent muls, then the<br />second one will follow one clk after the first. So we see the same<br />&#039;telescoping&#039; or &#039;waterfall&#039; effect...<br />1 mul takes 4 clks<br />2 muls take 5 clks<br />3 muls take 6 clks<br />etc.<br /><br />After execution, the µOps wait in the ROB for &quot;retirement&quot;. The retire stage<br />will update all operands (register or memory) in order, at a maximum of 3 µOps<br />per clk. It will also take care of the operand forwarding.<br /><br />This rather complex and very interesting execution scheme shifts the focus<br />from instruction scheduling to issuing µOps to the ooo backend. Scheduling<br />instructions for dependency is now less important, because the CPU can<br />schedule the µOps itself. There should not be so many dependencies that the<br />ooo-buffer fills up faster than the backend can handle, but not each and every<br />dependency results directly in a stall anymore. The ooo buffer acts like a<br />cushion for dependencies and latencies.<br /><br />You could see the CPU as a funnel for instructions... You can add up to 6<br />µOps per clk, then it can dispatch up to 5 µOps, and finally retire up to 3<br />µOps per clk.<br />When writing code for PPRO CPUs, the main focus should be on using as little<br />µOps as possible, rather than getting as many µOps per clk into the ooo<br />buffer. Decoding them efficiently is important aswell. Instructions that<br />consist of more than 1 µOp should be scheduled to go into d0. Try to get both<br />d1 and d2 to decode an instruction aswell.<br /><br />One part we&#039;ve not touched yet, is the PPRO&#039;s register renaming scheme. While<br />on the outside it appears that the PPRO has only 8 integer registers, it has<br />in fact 40 temporary registers inside. It uses these temporary registers<br />internally, for a special kind of dependencies such as this one:<br /><br /><pre><code>&nbsp; &nbsp; mov eax, <br />&nbsp; &nbsp; mul <strong><br />&nbsp; &nbsp; mov , eax<br />&nbsp; &nbsp; mov eax, </code></pre><br /><br />We saw earlier that a mul takes 4 clks. If the CPU would use the physical<br />registers directly, then the last mov would have to wait until the previous<br />store operation was dispatched. But instead the first three instructions get<br />one temporary register assigned for eax, and the last instruction gets a new<br />temporary register assigned again. The eax register has been &#039;renamed&#039; to a<br />temporary register. Now the load operation can be dispatched at any time, it<br />does not have to wait for the previous instructions to finish. The CPU can<br />rename 3 registers per clk. Each temporary register has its own dependency<br />chain. Try to keep these chains short, so that the CPU can allocate new<br />temporary registers early, which will reduce dependencies, and allow the CPU<br />to dispatch more indepentent µOps.<br />(note: the old tricks of &#039;xor reg, reg&#039; or &#039;sub reg, reg&#039; to set a register<br /> to 0 will not be recognized as independent. A new independent chain will not<br /> be created.)</div>
    <div class="meta">Posted on 2011-07-21 11:58:19 by Scali</div>
   </div>
  </div>
 </body>
</html>