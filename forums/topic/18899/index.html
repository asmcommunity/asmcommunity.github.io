<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8" />
  <title>Back-Propagation in FFNN - Forums - ASM Community</title>
  <link rel="stylesheet" type="text/css" href="../../../style.css" />
  <link rel="canonical" href="../?id=18899" />
     </head>
 <body>
  <div id="header">
   <h1><a href="../../../">ASM Community</a></h1>
  </div>
  <div id="content">
   <p class="breadcrumbs"><a href="../../../">Home</a> &raquo; <a href="../../">Forums</a> &raquo; <a href="../../board/?id=3">MAIN</a> &raquo; <a href="../?id=18899">Back-Propagation in FFNN</a></p>
   <div class="post" id="post-146248">
    <div class="subject"><a href="#post-146248">Back-Propagation in FFNN</a></div>
    <div class="body">I just want to make sure I understand this correctly.<br />This pertains to training a Feed-Forward Neural Network using back-propagation,<br /><br />After I have calculated the output set of floats, I should then calculate a deltaError set of floats by subtraction with a desired set of output floats.<br /><br />Then I should work back through the neural network, and for each neuron, correction of weights follows this basic formula:<br /><br />Neuron.CorrectedInputWeight=Neuron.InputWeight - (LearningCoefficient * (Neuron.InputWeight * deltaError) )<br /><br />so if the LearningCoefficient was 0.50f (half) then:<br />New weight = old weight - (half the old weight's contribution to the error)<br /><br />We simply work back through the layers of neurons one at a time, calculating their new input weights.<br /><br />Once we have calculated each layer's new weights in this fashion, the deltaError values exposed to the previous layer (as we work backwards) should now be recalculated from the corrected Input Weights of the current layer, rather than the ones we calculated initially at the output layer.<br /><br />When we reach (and have processed) the Input Layer of neurons, there's no more per-neuron Input Weights to correct, and we have completed the training cycle.<br /><br />Is all of this correct?</div>
    <div class="meta">Posted on 2004-07-17 08:06:15 by Homer</div>
   </div>
   <div class="post" id="post-146250">
    <div class="subject"><a href="#post-146250">Back-Propagation in FFNN</a></div>
    <div class="body">I think maybe you have the delta bit wrong.  The delta for an inner layer in the product of the deltas of the previous layers together with its own output weights. In effect the deltas travel back through the net in the same ways as input travel forward trhough it with the difference being that you're working out weighted products rather than sums.<br /><br />For large nets these weighted products quickly tend to 0 cause most of the numbers you deal with are less than 1.<br /><br />Unless your talking about some sort of Feedforward net which I haven't heard of? What I describe above is the classic backpropagation algo.</div>
    <div class="meta">Posted on 2004-07-17 08:47:19 by Eóin</div>
   </div>
   <div class="post" id="post-146252">
    <div class="subject"><a href="#post-146252">Back-Propagation in FFNN</a></div>
    <div class="body">yes, classic ff with back prop .. <br />if I post my version of this stuff, wanna help get the trainer working? the code is quite small - written with atc oop :)</div>
    <div class="meta">Posted on 2004-07-17 09:28:15 by Homer</div>
   </div>
   <div class="post" id="post-146254">
    <div class="subject"><a href="#post-146254">Back-Propagation in FFNN</a></div>
    <div class="body"><div class="quote">The delta for an inner layer in the product of the deltas of the previous layers together with its own output weights.</div><br /><br />So I calculate a SINGLE &quot;back-delta&quot; float value for the ENTIRE output set of neurons by adding their deltas, then I apply this back-delta value as the delta for all neurons of the previous layer?<br />You say product - do you mean sum, or do you mean I should multiply? I want help, I'm REALLY close here :)</div>
    <div class="meta">Posted on 2004-07-17 09:51:43 by Homer</div>
   </div>
   <div class="post" id="post-146257">
    <div class="subject"><a href="#post-146257">Back-Propagation in FFNN</a></div>
    <div class="body">Well I've no problem helping, but only have web access till tommorow eve.  Yes by product I do mean multiply, I learned the formuls from a site generation5.org (origionally pointed to it by thomas)  To sumarize the delta bit..<br /><br />First you do work out seperate delta for each output neuron in the manner you mention in your first post. This leaves you with each neuron having a particular delta representing how wrong it was, these deltas have to be propagated backwards using this formula<br /><br />d = x2(1 - x)w1d1w2d2...widi <br /><br />Where the d#'s represent the deltas of the layer above and the w#'s the weight connecting that neuron to the current.<br /><br />Only after propagating the deltas back do you then go and adjust the weights of each neuron and you adjust that weight by calculating (delta*lCoeff*input) for each weight and adding that value to the weight. I should stress <u>adding</u>  because I notice you mention subtraction in your first post.</div>
    <div class="meta">Posted on 2004-07-17 11:47:24 by Eóin</div>
   </div>
   <div class="post" id="post-146267">
    <div class="subject"><a href="#post-146267">Back-Propagation in FFNN</a></div>
    <div class="body">My code differs slightly to thomas' code.<br />It performs sigma filtering etc.<br /><br />There may be a couple of files in here I am not using in this example, let me know if anything is missing.</div>
    <div class="meta">Posted on 2004-07-17 13:11:42 by Homer</div>
   </div>
   <div class="post" id="post-146275">
    <div class="subject"><a href="#post-146275">Back-Propagation in FFNN</a></div>
    <div class="body">Sorry EvilHomer no offence to your code which seem nice but my head has been a bit fried of late and I'm not actually up to the challange of reading and trying to figure out code. I'm happy to try and answer questions or to maybe look at specific bits of code but I'm not currently able to apply myself to understanding something as a whole. Sorry.<br /><br />BTW what do you mean by simga filtering or how do you implement it?</div>
    <div class="meta">Posted on 2004-07-17 14:44:41 by Eóin</div>
   </div>
   <div class="post" id="post-146276">
    <div class="subject"><a href="#post-146276">Back-Propagation in FFNN</a></div>
    <div class="body">Sigma filtering is just a nice way of saying &quot;S-shaped graph&quot; - basically its a filter function we apply to smooth out harsh transitions from one side of zero to the other.<br /><br />The sigmoid function I have looks like this:<br />return ( 1 / ( 1 + exp(-netinput / response)))<br /><br />here's some notes to explain sigma better:<br /><a target="_blank" href="http://ieee.uow.edu.au/~daniel/software/libneural/BPN_tutorial/BPN_English/BPN_English/node5.html">http://ieee.uow.edu.au/~daniel/software/libneural/BPN_tutorial/BPN_English/BPN_English/node5.html</a></div>
    <div class="meta">Posted on 2004-07-17 14:48:47 by Homer</div>
   </div>
   <div class="post" id="post-146278">
    <div class="subject"><a href="#post-146278">Back-Propagation in FFNN</a></div>
    <div class="body">Do you perhaps mean Sigmoid? What is the response variable in the above equation? Sorry for all the question :) .</div>
    <div class="meta">Posted on 2004-07-17 15:19:42 by Eóin</div>
   </div>
   <div class="post" id="post-146283">
    <div class="subject"><a href="#post-146283">Back-Propagation in FFNN</a></div>
    <div class="body">I believe that refers to the ActivationResponse threshold per neuron.<br />The function takes the net Activation Input for a neuron and tempers it with the ActivationResponse value, while also hard-limiting it to unit scale (normalizes).<br /><br />afaik &quot;sigmoid&quot; describes any mathematical function with a sigma-wave output. A Sigma-wave is 0 at 0, 1 at infinity, and curves in an S shape inbetween jusimilar to the first half of sinewave.<br /><br />Anyhow, I found an article by that (Mathew James?) chap, where he describes a per-layer method of back-propagation very similar to how I described it, and says that he prefers it. I think I'll give it a go. The main point to note is that my NN has N outputs, not just one, otherwise it's straightforward.</div>
    <div class="meta">Posted on 2004-07-17 16:34:39 by Homer</div>
   </div>
   <div class="post" id="post-146284">
    <div class="subject"><a href="#post-146284">Back-Propagation in FFNN</a></div>
    <div class="body">btw, this project is the debut for my CArrayManager class, which is CVector:&lt;anything&gt; class - dynamic array manager for arrays with elements of arbitrary size :)</div>
    <div class="meta">Posted on 2004-07-17 16:39:42 by Homer</div>
   </div>
   <div class="post" id="post-146285">
    <div class="subject"><a href="#post-146285">Back-Propagation in FFNN</a></div>
    <div class="body">Yes, from the output layer working back, he calculates an error delta array for each layer, then (this confuses me) he adds and averages the error for each layer, and uses this common error metric to modify the input weights of ALL neurons on that layer, then moving back to the previous layer and so on.<br />I want to know why I would want to affect a neuron with the errors of its neighbours - this seems weird and wrong.<br />Should I not just propagate each output neuron's error back thru THAT neuron's input weights and so on? This would seem to be more logical to me, but then I'm probably very wrong here.</div>
    <div class="meta">Posted on 2004-07-17 16:44:53 by Homer</div>
   </div>
   <div class="post" id="post-146287">
    <div class="subject"><a href="#post-146287">Back-Propagation in FFNN</a></div>
    <div class="body">I agree with you that doesn't make sense.  It is important to note that if you modify the weights of one layer before working out the deltas of the previous layer then the deltas you later calculate there will be wrong. Not totally wrong probably but still...<br /><br />The thing with these nets is that most learning occurs in the final layer, thats why many different techniques still seem to produce Nets that can learn. But for highly non-liner associations between input and output then the extra layers are needed. As best I can tell the initial random weights scramble the inputs and the hope is that somewhere in the outputs of the hidden layers will be a pattern which can be linked linearly to the desired output. Traning then both makes this link and strenghtens that fluke pattern.<br /><br />The back propagation algorithim is mathematically worked out and hence you have bits in the formulas like x(1-x) which don't necessarly make intutive sense until you start working out integrals/derivatives of the sigmoid function. When you start playing with the formulas from a programming point of view you can find that everything still seems to work but chances are you'll mess the part of training which strenghtens those non-liner assocations and ultimately I've found that such nets then tend to be very bad at fine tuning their outputs to the desired ones.<br /><br />Also regards that response variable, Unless there is a very specific need for it i'd drop it. netinput is a weighted sum, dividing that by the response variable is only throwing another weighting into the mix. Unless the training equations take it into account then I could see such a variable slowing down learning. Even if they do take it into account I can't think of a way it be an improvement.<br /><br />I would be very interested in reading this article you mention.</div>
    <div class="meta">Posted on 2004-07-17 17:22:20 by Eóin</div>
   </div>
   <div class="post" id="post-146288">
    <div class="subject"><a href="#post-146288">Back-Propagation in FFNN</a></div>
    <div class="body"><a target="_blank" href="http://generation5.org/content/1999/nn_bp.asp">http://generation5.org/content/1999/nn_bp.asp</a></div>
    <div class="meta">Posted on 2004-07-17 17:41:37 by Homer</div>
   </div>
   <div class="post" id="post-146289">
    <div class="subject"><a href="#post-146289">Back-Propagation in FFNN</a></div>
    <div class="body">Thanks, I must say I disagree with bits of that, or rather I disagree with my understanding of it. He says that the formulas for the deltas are derived with respect of the squared error. Fair enough I believe he's right there. But there is certainly no need to actually work out what that squared error is cause it never used in the formulas, you might only use it to compare to a threshold value (as he does mention earlier).<br /><br />And in as much as I understand whats going on I can say that the following quote is wrong.<br /><br /><em>&quot;I prefer adjusting the weights one layer at a time. This method involves recomputing the network error before the next weight layer error terms are computed.&quot;</em><br /><br />The errors of neurons in the hidden layer tell how much they contributed to the overall error. If you have since modified the weights then those errors cannot be calculated correctly and the deltas based on them will also be wrong.</div>
    <div class="meta">Posted on 2004-07-17 18:06:40 by Eóin</div>
   </div>
   <div class="post" id="post-146290">
    <div class="subject"><a href="#post-146290">Back-Propagation in FFNN</a></div>
    <div class="body">I can't see why I can't do the following:<br />-calculate deltas for current neuron layer based on current layer outputs and &quot;deltas from the previous layer&quot; (except for the output layer, where we use the difference between output and training set)<br />-alter the weights of the current neuron layer according to the  input-weighted partial derivatives of the delta error per neuron<br />(and NOT based on the weighted derivative of the network error squared sum)<br />-move back one layer and repeat<br /><br />Seems to me that once we have our deltas per layer, we are ready to modify the weights for that layer, provided we hand the deltas we calculated to the next earlier layer in the next iteration.<br />There seems no reason to calculate deltas for all layers before modifying the weights.</div>
    <div class="meta">Posted on 2004-07-17 18:13:08 by Homer</div>
   </div>
   <div class="post" id="post-146301">
    <div class="subject"><a href="#post-146301">Back-Propagation in FFNN</a></div>
    <div class="body">When you calculate the deltas for one of the hidden layers you need the deltas of the previous layer, but in the calculation those deltas are multiplied by the weights which connect those layers, if you had previously adjusted those weights then the deltas wil be different.<br /><br />I can understand the logic that the article seems to suggest that the adjusted weights are more correct and so any deltas calculated with them should also be more correct but I don't think it works thats way. To be honest I don't know this for an absolute fact but if you imagine a situation where a weight gets adjusted from a positive to a negative value, the the deltas of the previous layers will be opposite sign of they would have been. <br /><br />Maybe this really would be more correct, but without seeing some research on the topic I would place my trust in the classic algorithim...</div>
    <div class="meta">Posted on 2004-07-18 05:48:42 by Eóin</div>
   </div>
  </div>
 </body>
</html>