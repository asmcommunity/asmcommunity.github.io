<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8" />
  <title>MMX on P4 (and AMD,P3,PMMX) - Forums - ASM Community</title>
  <link rel="stylesheet" type="text/css" href="../../../style.css" />
  <link rel="canonical" href="../?id=3962" />
     </head>
 <body>
  <div id="header">
   <h1><a href="../../../">ASM Community</a></h1>
  </div>
  <div id="content">
   <p class="breadcrumbs"><a href="../../../">Home</a> &raquo; <a href="../../">Forums</a> &raquo; <a href="../../board/?id=3">MAIN</a> &raquo; <a href="../?id=3962">MMX on P4 (and AMD,P3,PMMX)</a></p>
   <div class="post" id="post-26927">
    <div class="subject"><a href="#post-26927">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">Got my hands finally on a Pentium 4 (1.4 GHz).<br /><br />In the thread ?Quickk reference to MMX/SSE/SSE2 ...&quot; ...<br /><br /><a target="_blank" href="http://www.asmcommunity.net/board/index.php?topic=3708">http://www.asmcommunity.net/board/index.php?topic=3708</a><br /><br />... bitRAKE optimized a mmx-3D-transform-loop from intel (see below) from 15 down to only 10 cyles on AMD Athlon (data from/to register).<br /><br />Now we have the cycles/loop for 4 Processors:<br /><br />1) PMMX: 12 cycles <br />2) P3: 12 cycles<br />3) AMD Athlon: 10 cycles<br /><br />and:<br /><br />4)  P4: 17 cycles !!!<br /><br />If there is no catch then it looks like that the marketing-department from intel is now involved in research and development too...<br /><br />There are exactly 17 MMX-instructions in the loop so the statement (link above) ... :<br /><br />?But troubles do not stop here. The image above outlines how instructions are addressed to specific ports in the P4 execution engine. All MMX instructions are queued in Port 1! This is major drawback compared to the P6 Core, in which most MMX instructions could be issued to Port 0 or Port 1. ? <br /><br />... seems to be true.<br /><br />Consequence? Enjoy your machine (and wait for the hammer).<br /><br />The loop (3D-transforms vectors xyzw, each 16 bit):<br /><br /><br />---------------------------<br />NextVect:<br />        ; Load vector (4 16-bit elements) into reg<br /><br />        movq mm3, mm7  ;original: movq mm3,<br /><br />        movq    mm4,mm3<br /><br />        pmaddwd mm3,mm0<br />        movq    mm5,mm4<br /><br />        pmaddwd mm4,mm1<br />        inc     ecx<br /><br />        pmaddwd mm5,mm2<br />        movq    mm6,mm3<br /><br />        punpckldq mm3,mm4<br /><br />        punpckhdq mm6,mm4<br /><br />        movq    mm4,mm5<br />        punpckhdq mm5,mm5<br /><br />        paddd   mm3,mm6<br />        paddd   mm5,mm4<br /><br />        psrad   mm3,AnzNKomma<br />        psrad   mm5,AnzNKomma<br /><br />        packssdw mm3,mm5<br />        movq mm4, mm3 ; original: movq ,mm3<br /><br />        jnz     NextVect<br />---------------------------<br /><br /> <br /><br /><br />Rem.: <br />---quote bitRAKE---------<br /> ?I figured it out - these dummy test runs don't work on out-of-order processors. The dummy moves to MM7 were working against the internal optimizers within the core of the processor&quot;<br />-----------------------------<br /><br />bitRAKE: What was the issue with reading/writing the vectors from/to mmx-registers (On P4 it was exactly the codesniped from above)?<br /><br /><br />VShader</div>
    <div class="meta">Posted on 2002-03-03 10:39:39 by VShader</div>
   </div>
   <div class="post" id="post-26931">
    <div class="subject"><a href="#post-26931">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body"><strong>VShader</strong>, the problem is that the Athlon optimizes the MacroOps internally, and the dummy moves create false dependancies - preventing optimization.<br /><br />Intel really wants people to use SSE/2, because the Athlon so thoroughly beats the P3/P4 at MMX.  I read up on both chips internal workings before buying an Athlon.  It performs as expected - the only problem is keeping the CPU fed with data from memory.  And this is the problem that the P4 tried to solve - on a 2.2Ghz chip, 17 cycles isn't a problem because memory is so slow.  Need to use prefetch to speed up real world use of algos, because the weakest link isn't the CPU - it is memory.  This is why no speed increase is really precieved between these very fast CPUs and those slightly faster than half the speed - most applications aren't optimized for that preformance range.<br /><br />What timing did the code in <a target="_blank" href="http://www.asmcommunity.net/board/index.php?topic=3708.msg25266">( this )</a> post show?</div>
    <div class="meta">Posted on 2002-03-03 11:53:22 by bitRAKE</div>
   </div>
   <div class="post" id="post-26940">
    <div class="subject"><a href="#post-26940">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">Have only tested the sequence above and I am not sure if I can test again because my colleague wrote something about an explosion and reformatting her harddisk - but I think she is joking.<br /><br />VShader</div>
    <div class="meta">Posted on 2002-03-03 12:44:45 by VShader</div>
   </div>
   <div class="post" id="post-26956">
    <div class="subject"><a href="#post-26956">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">btw: Some very cool and in depth information on Pentium4 and other Intel-processors can be found here:<br /><br /><a target="_blank" href="http://www.emulators.com/pentium4.htm">http://www.emulators.com/pentium4.htm</a><br /><br /><br />VShader</div>
    <div class="meta">Posted on 2002-03-03 13:37:41 by VShader</div>
   </div>
   <div class="post" id="post-26958">
    <div class="subject"><a href="#post-26958">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">My tests show that if the vectors aren't in the cache, then each transform takes ~40 cycles.  Those 30 extra cycles are based on a 1.3Ghz CPU with DDR memory - expect this to be higher on faster processors.  This leaves quite some room for prefetch optimizations!  The ideal design would be one where blocks of vectors are processed from start to finish in small batches - where all the memory read/write takes place in cache, and no prefetching is needed.  The block size would be based on the data cache size.  Most processes can't fit into such a tight design constraint, so work is still to be done on the above algo to even get close to the 10 cycle minimum, and the final product will be processor dependant.  :(<br /><br />Thanks for the link.</div>
    <div class="meta">Posted on 2002-03-03 13:48:57 by bitRAKE</div>
   </div>
   <div class="post" id="post-26976">
    <div class="subject"><a href="#post-26976">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">All the info on the P4 suggests Intel has been working on the Prestonia Hyper-Threading for a very long time, and everything in the design is geared towards that feature.  The P4 isn't the best on the block right now, but maybe in the short future it will be a different story?  I imagine the Prestonia running as four concurrent processors at 2Ghz, and leaving the Athlon in the dust.  The main thing to come from this is that Mhz/Ghz doesn't mean much anymore. :)</div>
    <div class="meta">Posted on 2002-03-03 14:39:09 by bitRAKE</div>
   </div>
   <div class="post" id="post-27079">
    <div class="subject"><a href="#post-27079">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">I don't think MHz has been an exact indicator since the 486 processer. I believe artificial clock cycles were introduced to that processor.<br />Actually, as processors have progressed past the the original 8086, generally necessary clock cycles per instruction went down, with a few exceptions.<br /><br />Dig</div>
    <div class="meta">Posted on 2002-03-03 22:43:49 by dig</div>
   </div>
   <div class="post" id="post-27103">
    <div class="subject"><a href="#post-27103">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">I just found this page about Athlon instruction timing experiments:<br /><a target="_blank" href="http://members.jcom.home.ne.jp/kgoto/athlon.html">http://members.jcom.home.ne.jp/kgoto/athlon.html</a><br /><br />Using prefetch and MOVNTQ, I have only been able to get the timing down to 30 cycles per vector average of 100,000 vectors.  This is down from 45 cycles - before when I stated 40, I was reading/writing to the same memory.  That is still 66% of the time spent waiting on memory!<br /><br /><strong>Edit</strong>: Adding a 32K 'touch' prefetch on the source has helped drop the timing to &lt;24 cycles per vector - again 100,000 vectors not in cache.<br /><br />Very solid timing of 24 +/- 0.5 cycles per vector:<pre><code>	mov	ecx,iNumVec<br />	mov	eax,pMatrix<br />	lea edx,&#91;ecx*8&#93;<br />	neg ecx<br /><br />	movq mm0,&#91;eax +  0&#93;<br />	movq mm1,&#91;eax +  8&#93;<br />	movq mm2,&#91;eax + 16&#93;<br /><br />	mov eax,edx<br />	add	edx,pVector<br />	add	eax,pResult<br /><br />_32k_prefetch&#58;<br />	mov ebx,&#40;32*1024/64&#41;/2<br />	; unrolling this more than two doesn't help<br />@@&#58;	mov esi,&#91;edx + ecx*8 + 64*0&#93;<br />	mov edi,&#91;edx + ecx*8 + 64*1&#93;<br />	add ecx, 64*2/8<br />	dec ebx<br />	jne @B<br />	mov ebx, ecx<br />	sub ecx, 64*2/8 * &#40;32*1024/64&#41;/2<br /><br />NextVect&#58;<br />	cmp ecx, ebx<br />	je	_32k_prefetch<br /><br />	movq	mm3,&#91;edx + ecx*8&#93;<br />	inc	ecx<br /><br />	movq	mm4,mm3<br />	pmaddwd	mm3,mm0<br /><br />	movq	mm5,mm4<br />	pmaddwd	mm4,mm1<br /><br />	movq	mm6,mm3<br />	pmaddwd	mm5,mm2<br /><br />	punpckldq mm3,mm4<br />	punpckhdq mm6,mm4<br /><br />	movq	mm4,mm5<br />	punpckhdq mm5,mm5<br /><br />	paddd	mm3,mm6<br />	paddd	mm5,mm4<br /><br />	psrad	mm3,NUMBER_SCALE-2<br />	psrad	mm5,NUMBER_SCALE-2<br /><br />	packssdw mm3,mm5<br />	movntq	&#91;eax + ecx*8 - 8&#93;,mm3<br /><br />	jnz	NextVect<br /><br />	sfence<br />	emms</code></pre></div>
    <div class="meta">Posted on 2002-03-04 02:39:15 by bitRAKE</div>
   </div>
   <div class="post" id="post-27207">
    <div class="subject"><a href="#post-27207">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body">bitRAKE, <br /><br />The w-component (16bit) of the untransformed xyzw mmx-vector ist always 1.0 (fixed point).<br /><br />So you can (on a faster processor with more cycles to spend) store the vertices in only 6 bytes (xyz xyz xyz xyz ...) instead of 8 (xyzw ...)  and insert the 1.0 while transforming to chop off another 25% of input-data.<br /><br />BTW: With a bit of loopunrolling and EDO-Ram friendly reading (5-2-2-2 Burst) I have now about 10.000.000 vectortransforms/s (while running the 3D-engine) and 20 cycles per vector on my P200mmx - not fair (because you have more Hz) , but: go for it! :grin:<br /><br />VShader</div>
    <div class="meta">Posted on 2002-03-04 16:15:55 by VShader</div>
   </div>
   <div class="post" id="post-27220">
    <div class="subject"><a href="#post-27220">MMX on P4 (and AMD,P3,PMMX)</a></div>
    <div class="body"><strong>VShader</strong>, this has been a good test for me.  It's demostrated what kind of performance hit is taken by non-cached data on newer processors, and now much time is spent moving data around.  My approach to data proccessing will be different in the future.  Thanks for the idea, I'll see what I can do with it.</div>
    <div class="meta">Posted on 2002-03-04 17:22:29 by bitRAKE</div>
   </div>
  </div>
 </body>
</html>